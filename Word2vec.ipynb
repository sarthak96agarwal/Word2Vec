{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json('reddit_jokes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Now I have to say \"Leroy can you please paint ...</td>\n",
       "      <td>5tz52q</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate how you cant even say black paint anymore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pizza doesn't scream when you put it in the ov...</td>\n",
       "      <td>5tz4dd</td>\n",
       "      <td>0</td>\n",
       "      <td>What's the difference between a Jew in Nazi Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...and being there really helped me learn abou...</td>\n",
       "      <td>5tz319</td>\n",
       "      <td>0</td>\n",
       "      <td>I recently went to America....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Sunday school teacher is concerned that his ...</td>\n",
       "      <td>5tz2wj</td>\n",
       "      <td>1</td>\n",
       "      <td>Brian raises his hand and says, “He’s in Heaven.”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He got caught trying to sell the two books to ...</td>\n",
       "      <td>5tz1pc</td>\n",
       "      <td>0</td>\n",
       "      <td>You hear about the University book store worke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body      id  score  \\\n",
       "0  Now I have to say \"Leroy can you please paint ...  5tz52q      1   \n",
       "1  Pizza doesn't scream when you put it in the ov...  5tz4dd      0   \n",
       "2  ...and being there really helped me learn abou...  5tz319      0   \n",
       "3  A Sunday school teacher is concerned that his ...  5tz2wj      1   \n",
       "4  He got caught trying to sell the two books to ...  5tz1pc      0   \n",
       "\n",
       "                                               title  \n",
       "0   I hate how you cant even say black paint anymore  \n",
       "1  What's the difference between a Jew in Nazi Ge...  \n",
       "2                     I recently went to America....  \n",
       "3  Brian raises his hand and says, “He’s in Heaven.”  \n",
       "4  You hear about the University book store worke...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=df['body'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize_text(text):\n",
    "\n",
    "    text=text.lower()\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(pic\\.twitter\\.com/[^\\s]+))','', text)\n",
    "    text = re.sub('@[^\\s]+','', text)\n",
    "    text = re.sub('#([^\\s]+)', '', text)\n",
    "    text = re.sub('[:;>?<=*+()&,\\-#!$%\\{˜|\\}\\[^_\\\\@\\]1234567890’‘]',' ', text)\n",
    "    text = re.sub('[\\d]','', text)\n",
    "    text = text.replace(\".\", '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = text.replace(\"`\", '')\n",
    "    text = text.replace(\"'s\", '')\n",
    "    text = text.replace(\"/\", ' ')\n",
    "    text = text.replace(\"\\\"\", ' ')\n",
    "    text = text.replace(\"\\\\\", '')\n",
    "    re.sub(' +', ' ', text)\n",
    "    text=text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "    #normalize some utf8 encoding\n",
    "    text = text.replace(\"\\x9d\",'').replace(\"\\x8c\",'')\n",
    "    text = text.replace(\"\\xa0\",'')\n",
    "    text = text.replace(\"\\x9d\\x92\", '').replace(\"\\x9a\\xaa\\xf0\\x9f\\x94\\xb5\", '').replace(\"\\xf0\\x9f\\x91\\x8d\\x87\\xba\\xf0\\x9f\\x87\\xb8\", '').replace(\"\\x9f\",'').replace(\"\\x91\\x8d\",'')\n",
    "    text = text.replace(\"\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8\",'').replace(\"\\xf0\",'').replace('\\xf0x9f','').replace(\"\\x9f\\x91\\x8d\",'').replace(\"\\x87\\xba\\x87\\xb8\",'')\n",
    "    text = text.replace(\"\\xe2\\x80\\x94\",'').replace(\"\\x9d\\xa4\",'').replace(\"\\x96\\x91\",'').replace(\"\\xe1\\x91\\xac\\xc9\\x8c\\xce\\x90\\xc8\\xbb\\xef\\xbb\\x89\\xd4\\xbc\\xef\\xbb\\x89\\xc5\\xa0\\xc5\\xa0\\xc2\\xb8\",'')\n",
    "    text = text.replace(\"\\xe2\\x80\\x99s\", \"\").replace(\"\\xe2\\x80\\x98\", '').replace(\"\\xe2\\x80\\x99\", '').replace(\"\\xe2\\x80\\x9c\", \"\").replace(\"\\xe2\\x80\\x9d\", \"\")\n",
    "    text = text.replace(\"\\xe2\\x82\\xac\", \"\").replace(\"\\xc2\\xa3\", \"\").replace(\"\\xc2\\xa0\", \"\").replace(\"\\xc2\\xab\", \"\").replace(\"\\xf0\\x9f\\x94\\xb4\", \"\").replace(\"\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8\\xf0\\x9f\", \"\")\n",
    "    text =  re.sub(r\"\\b[a-z]\\b\", \"\", text)\n",
    "    text=re.sub( '\\s+', ' ', text).strip()\n",
    "    \n",
    "    text=re.sub(r'\\.+', \".\", text)\n",
    "    text=re.sub(r'\\.\\.+', ' ', text).replace('.', '')\n",
    "    # Replace multiple dots with space\n",
    "    text = re.sub('\\.\\.+', ' ', text) \n",
    "    # Remove single dots\n",
    "    text = re.sub('\\.', '', text)\n",
    "    text = re.sub(r'\\.{2,}', ' ', text)\n",
    "    text = re.sub(r'\\.{1}', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_text=[]\n",
    "for sentence in df['body']:\n",
    "    normed_text.append(normalize_text(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tflearn/data_utils.py:201: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "max_sent_length = 50\n",
    "#make vocab processor\n",
    "vocab_processor = tflearn.data_utils.VocabularyProcessor(max_sent_length)\n",
    "# our sentences represented as indices instead of words\n",
    "data = list(vocab_processor.fit_transform(normed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab_processor.vocabulary_)\n",
    "#convert numpy lists to python lists\n",
    "data = [i.tolist() for i in data]\n",
    "##get rid of trailing 0s\n",
    "for lst in data:\n",
    "    try:\n",
    "        ##pop off last index if it is equal to 0\n",
    "        while lst[-1] == 0:\n",
    "            lst.pop()\n",
    "    except:\n",
    "        pass\n",
    "#filter the empty lists\n",
    "data = filter(None, data)\n",
    "#convert data to numpy list after converting filter to python list\n",
    "data = np.array(list(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "window=2\n",
    "pivot_words = []\n",
    "##target words i.e. our y's or outputs\n",
    "target_words = []\n",
    "# data shape = [num sentences, sentence_length]\n",
    "for d in range(data.shape[0]):\n",
    "    pivot_idx = data[d][window:-window]\n",
    "\n",
    "    for i in range(len(pivot_idx)):\n",
    "        #get the current pivot word\n",
    "        pivot = pivot_idx[i]\n",
    "\n",
    "        #targets array\n",
    "        targets = np.array([])\n",
    "\n",
    "        neg_target = data[d][i : window+i]\n",
    "        pos_target = data[d][i + window +1: i + window + window +1]\n",
    "        targets = np.append(targets, [neg_target, pos_target]).flatten().tolist()\n",
    "\n",
    "        for c in range(window*2):\n",
    "            pivot_words.append(pivot)\n",
    "            target_words.append(targets[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_vec():\n",
    "    embed_size=128\n",
    "    num_samples=64\n",
    "    learning_rate = 0.001\n",
    "    x=tf.placeholder(shape=[None,], dtype=tf.int32, name=\"x_center_id\")\n",
    "    y=tf.placeholder(shape=[None,], dtype=tf.int32, name=\"y_target_id\")\n",
    "    Embedding = tf.Variable(tf.random_uniform([vocab_size,embed_size],-1.0,1.0), name=\"word_embeddings\")\n",
    "    nce_weights= tf.Variable(tf.truncated_normal([vocab_size, embed_size],stddev=tf.sqrt(1/embed_size)), name=\"nce_weights\")\n",
    "    nce_baises = tf.Variable(tf.zeros([vocab_size]), name=\"nce_bias\")\n",
    "    pivot = tf.nn.embedding_lookup(Embedding, x, name=\"embedding_lookup\")\n",
    "    train_labels= tf.reshape(y, [tf.shape(y)[0], 1])\n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights, biases=nce_baises, labels=train_labels, inputs=pivot, num_sampled=num_samples, num_classes=vocab_size, num_true=1))\n",
    "    optimizer = tf.contrib.layers.optimize_loss(loss,tf.train.get_global_step(),learning_rate,\"Adam\",clip_gradients=5.0,name=\"Optimizer\")\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    return optimizer, loss, x, y, sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 100 of 237859 LOSS: 266.82706\n",
      "STEP 200 of 237859 LOSS: 319.50702\n",
      "STEP 300 of 237859 LOSS: 306.35516\n",
      "STEP 400 of 237859 LOSS: 329.1489\n",
      "STEP 500 of 237859 LOSS: 291.06677\n",
      "STEP 600 of 237859 LOSS: 225.70999\n",
      "STEP 700 of 237859 LOSS: 238.36084\n",
      "STEP 800 of 237859 LOSS: 232.93266\n",
      "STEP 900 of 237859 LOSS: 237.52977\n",
      "STEP 1000 of 237859 LOSS: 234.98282\n",
      "STEP 1100 of 237859 LOSS: 289.67047\n",
      "STEP 1200 of 237859 LOSS: 226.59872\n",
      "STEP 1300 of 237859 LOSS: 227.94543\n",
      "STEP 1400 of 237859 LOSS: 227.51407\n",
      "STEP 1500 of 237859 LOSS: 248.52731\n",
      "STEP 1600 of 237859 LOSS: 198.86066\n",
      "STEP 1700 of 237859 LOSS: 207.38565\n",
      "STEP 1800 of 237859 LOSS: 194.64322\n",
      "STEP 1900 of 237859 LOSS: 205.9089\n",
      "STEP 2000 of 237859 LOSS: 199.56845\n",
      "STEP 2100 of 237859 LOSS: 182.57047\n",
      "STEP 2200 of 237859 LOSS: 171.89236\n",
      "STEP 2300 of 237859 LOSS: 156.3905\n",
      "STEP 2400 of 237859 LOSS: 189.57378\n",
      "STEP 2500 of 237859 LOSS: 158.80524\n",
      "STEP 2600 of 237859 LOSS: 152.67465\n",
      "STEP 2700 of 237859 LOSS: 151.87714\n",
      "STEP 2800 of 237859 LOSS: 163.84314\n",
      "STEP 2900 of 237859 LOSS: 126.68979\n",
      "STEP 3000 of 237859 LOSS: 179.84671\n",
      "STEP 3100 of 237859 LOSS: 134.94019\n",
      "STEP 3200 of 237859 LOSS: 135.33206\n",
      "STEP 3300 of 237859 LOSS: 107.99868\n",
      "STEP 3400 of 237859 LOSS: 159.38774\n",
      "STEP 3500 of 237859 LOSS: 148.84842\n",
      "STEP 3600 of 237859 LOSS: 140.96529\n",
      "STEP 3700 of 237859 LOSS: 133.44737\n",
      "STEP 3800 of 237859 LOSS: 163.35805\n",
      "STEP 3900 of 237859 LOSS: 150.2302\n",
      "STEP 4000 of 237859 LOSS: 115.38038\n",
      "STEP 4100 of 237859 LOSS: 133.85184\n",
      "STEP 4200 of 237859 LOSS: 117.37889\n",
      "STEP 4300 of 237859 LOSS: 132.42494\n",
      "STEP 4400 of 237859 LOSS: 138.41917\n",
      "STEP 4500 of 237859 LOSS: 109.64366\n",
      "STEP 4600 of 237859 LOSS: 94.297424\n",
      "STEP 4700 of 237859 LOSS: 108.31611\n",
      "STEP 4800 of 237859 LOSS: 123.442184\n",
      "STEP 4900 of 237859 LOSS: 93.6604\n",
      "STEP 5000 of 237859 LOSS: 122.20002\n",
      "STEP 5100 of 237859 LOSS: 102.90349\n",
      "STEP 5200 of 237859 LOSS: 113.62802\n",
      "STEP 5300 of 237859 LOSS: 102.337524\n",
      "STEP 5400 of 237859 LOSS: 99.040794\n",
      "STEP 5500 of 237859 LOSS: 94.98167\n",
      "STEP 5600 of 237859 LOSS: 119.85909\n",
      "STEP 5700 of 237859 LOSS: 68.95502\n",
      "STEP 5800 of 237859 LOSS: 104.72807\n",
      "STEP 5900 of 237859 LOSS: 91.97173\n",
      "STEP 6000 of 237859 LOSS: 96.38249\n",
      "STEP 6100 of 237859 LOSS: 110.94107\n",
      "STEP 6200 of 237859 LOSS: 105.23368\n",
      "STEP 6300 of 237859 LOSS: 60.717026\n",
      "STEP 6400 of 237859 LOSS: 95.39287\n",
      "STEP 6500 of 237859 LOSS: 94.14076\n",
      "STEP 6600 of 237859 LOSS: 73.75318\n",
      "STEP 6700 of 237859 LOSS: 92.19214\n",
      "STEP 6800 of 237859 LOSS: 62.59367\n",
      "STEP 6900 of 237859 LOSS: 79.869774\n",
      "STEP 7000 of 237859 LOSS: 103.13543\n",
      "STEP 7100 of 237859 LOSS: 71.31224\n",
      "STEP 7200 of 237859 LOSS: 95.3584\n",
      "STEP 7300 of 237859 LOSS: 101.23813\n",
      "STEP 7400 of 237859 LOSS: 86.43417\n",
      "STEP 7500 of 237859 LOSS: 125.72372\n",
      "STEP 7600 of 237859 LOSS: 83.44303\n",
      "STEP 7700 of 237859 LOSS: 122.09073\n",
      "STEP 7800 of 237859 LOSS: 56.22335\n",
      "STEP 7900 of 237859 LOSS: 62.496834\n",
      "STEP 8000 of 237859 LOSS: 46.425266\n",
      "STEP 8100 of 237859 LOSS: 97.14003\n",
      "STEP 8200 of 237859 LOSS: 83.00956\n",
      "STEP 8300 of 237859 LOSS: 56.485252\n",
      "STEP 8400 of 237859 LOSS: 67.927605\n",
      "STEP 8500 of 237859 LOSS: 101.09837\n",
      "STEP 8600 of 237859 LOSS: 100.877235\n",
      "STEP 8700 of 237859 LOSS: 44.055115\n",
      "STEP 8800 of 237859 LOSS: 61.131123\n",
      "STEP 8900 of 237859 LOSS: 83.264534\n",
      "STEP 9000 of 237859 LOSS: 85.7021\n",
      "STEP 9100 of 237859 LOSS: 42.221725\n",
      "STEP 9200 of 237859 LOSS: 65.259705\n",
      "STEP 9300 of 237859 LOSS: 33.888313\n",
      "STEP 9400 of 237859 LOSS: 57.149906\n",
      "STEP 9500 of 237859 LOSS: 72.07353\n",
      "STEP 9600 of 237859 LOSS: 89.29215\n",
      "STEP 9700 of 237859 LOSS: 55.57124\n",
      "STEP 9800 of 237859 LOSS: 65.62857\n",
      "STEP 9900 of 237859 LOSS: 60.472305\n",
      "STEP 10000 of 237859 LOSS: 106.41736\n",
      "STEP 10100 of 237859 LOSS: 69.740005\n",
      "STEP 10200 of 237859 LOSS: 67.30354\n",
      "STEP 10300 of 237859 LOSS: 56.202686\n",
      "STEP 10400 of 237859 LOSS: 59.70778\n",
      "STEP 10500 of 237859 LOSS: 86.616005\n",
      "STEP 10600 of 237859 LOSS: 84.73659\n",
      "STEP 10700 of 237859 LOSS: 71.41037\n",
      "STEP 10800 of 237859 LOSS: 74.461\n",
      "STEP 10900 of 237859 LOSS: 67.116905\n",
      "STEP 11000 of 237859 LOSS: 74.87991\n",
      "STEP 11100 of 237859 LOSS: 64.35354\n",
      "STEP 11200 of 237859 LOSS: 34.359795\n",
      "STEP 11300 of 237859 LOSS: 43.31944\n",
      "STEP 11400 of 237859 LOSS: 71.74335\n",
      "STEP 11500 of 237859 LOSS: 86.35898\n",
      "STEP 11600 of 237859 LOSS: 38.09467\n",
      "STEP 11700 of 237859 LOSS: 39.649723\n",
      "STEP 11800 of 237859 LOSS: 47.55282\n",
      "STEP 11900 of 237859 LOSS: 41.447758\n",
      "STEP 12000 of 237859 LOSS: 43.58326\n",
      "STEP 12100 of 237859 LOSS: 73.74367\n",
      "STEP 12200 of 237859 LOSS: 59.247818\n",
      "STEP 12300 of 237859 LOSS: 65.81526\n",
      "STEP 12400 of 237859 LOSS: 65.74591\n",
      "STEP 12500 of 237859 LOSS: 66.44977\n",
      "STEP 12600 of 237859 LOSS: 74.437874\n",
      "STEP 12700 of 237859 LOSS: 36.999554\n",
      "STEP 12800 of 237859 LOSS: 53.678577\n",
      "STEP 12900 of 237859 LOSS: 39.496895\n",
      "STEP 13000 of 237859 LOSS: 53.002686\n",
      "STEP 13100 of 237859 LOSS: 46.521385\n",
      "STEP 13200 of 237859 LOSS: 38.616127\n",
      "STEP 13300 of 237859 LOSS: 45.02288\n",
      "STEP 13400 of 237859 LOSS: 49.043495\n",
      "STEP 13500 of 237859 LOSS: 70.45886\n",
      "STEP 13600 of 237859 LOSS: 58.67376\n",
      "STEP 13700 of 237859 LOSS: 50.02117\n",
      "STEP 13800 of 237859 LOSS: 38.275833\n",
      "STEP 13900 of 237859 LOSS: 38.401054\n",
      "STEP 14000 of 237859 LOSS: 37.21221\n",
      "STEP 14100 of 237859 LOSS: 25.897774\n",
      "STEP 14200 of 237859 LOSS: 42.193336\n",
      "STEP 14300 of 237859 LOSS: 61.09176\n",
      "STEP 14400 of 237859 LOSS: 23.868523\n",
      "STEP 14500 of 237859 LOSS: 44.31185\n",
      "STEP 14600 of 237859 LOSS: 29.895054\n",
      "STEP 14700 of 237859 LOSS: 27.568829\n",
      "STEP 14800 of 237859 LOSS: 63.94893\n",
      "STEP 14900 of 237859 LOSS: 35.775505\n",
      "STEP 15000 of 237859 LOSS: 26.398127\n",
      "STEP 15100 of 237859 LOSS: 40.401863\n",
      "STEP 15200 of 237859 LOSS: 55.20575\n",
      "STEP 15300 of 237859 LOSS: 33.57715\n",
      "STEP 15400 of 237859 LOSS: 32.965256\n",
      "STEP 15500 of 237859 LOSS: 62.911106\n",
      "STEP 15600 of 237859 LOSS: 73.8027\n",
      "STEP 15700 of 237859 LOSS: 43.118286\n",
      "STEP 15800 of 237859 LOSS: 33.329994\n",
      "STEP 15900 of 237859 LOSS: 49.204025\n",
      "STEP 16000 of 237859 LOSS: 38.784126\n",
      "STEP 16100 of 237859 LOSS: 37.25934\n",
      "STEP 16200 of 237859 LOSS: 24.436966\n",
      "STEP 16300 of 237859 LOSS: 51.921104\n",
      "STEP 16400 of 237859 LOSS: 39.011692\n",
      "STEP 16500 of 237859 LOSS: 42.507782\n",
      "STEP 16600 of 237859 LOSS: 50.055107\n",
      "STEP 16700 of 237859 LOSS: 41.51114\n",
      "STEP 16800 of 237859 LOSS: 43.038464\n",
      "STEP 16900 of 237859 LOSS: 43.64205\n",
      "STEP 17000 of 237859 LOSS: 24.473131\n",
      "STEP 17100 of 237859 LOSS: 43.382355\n",
      "STEP 17200 of 237859 LOSS: 27.402113\n",
      "STEP 17300 of 237859 LOSS: 36.428734\n",
      "STEP 17400 of 237859 LOSS: 45.816284\n",
      "STEP 17500 of 237859 LOSS: 57.846924\n",
      "STEP 17600 of 237859 LOSS: 34.692642\n",
      "STEP 17700 of 237859 LOSS: 34.061752\n",
      "STEP 17800 of 237859 LOSS: 49.366333\n",
      "STEP 17900 of 237859 LOSS: 32.034626\n",
      "STEP 18000 of 237859 LOSS: 38.68673\n",
      "STEP 18100 of 237859 LOSS: 16.99806\n",
      "STEP 18200 of 237859 LOSS: 33.359737\n",
      "STEP 18300 of 237859 LOSS: 53.728245\n",
      "STEP 18400 of 237859 LOSS: 57.758408\n",
      "STEP 18500 of 237859 LOSS: 23.741198\n",
      "STEP 18600 of 237859 LOSS: 39.934456\n",
      "STEP 18700 of 237859 LOSS: 40.657097\n",
      "STEP 18800 of 237859 LOSS: 48.678288\n",
      "STEP 18900 of 237859 LOSS: 27.48224\n",
      "STEP 19000 of 237859 LOSS: 15.355232\n",
      "STEP 19100 of 237859 LOSS: 13.602921\n",
      "STEP 19200 of 237859 LOSS: 26.662708\n",
      "STEP 19300 of 237859 LOSS: 36.16528\n",
      "STEP 19400 of 237859 LOSS: 30.911886\n",
      "STEP 19500 of 237859 LOSS: 22.716917\n",
      "STEP 19600 of 237859 LOSS: 42.247345\n",
      "STEP 19700 of 237859 LOSS: 47.57657\n",
      "STEP 19800 of 237859 LOSS: 32.105408\n",
      "STEP 19900 of 237859 LOSS: 34.894966\n",
      "STEP 20000 of 237859 LOSS: 23.362883\n",
      "STEP 20100 of 237859 LOSS: 40.90925\n",
      "STEP 20200 of 237859 LOSS: 28.98814\n",
      "STEP 20300 of 237859 LOSS: 20.543674\n",
      "STEP 20400 of 237859 LOSS: 44.23215\n",
      "STEP 20500 of 237859 LOSS: 33.74678\n",
      "STEP 20600 of 237859 LOSS: 24.144371\n",
      "STEP 20700 of 237859 LOSS: 26.547789\n",
      "STEP 20800 of 237859 LOSS: 36.375675\n",
      "STEP 20900 of 237859 LOSS: 44.891693\n",
      "STEP 21000 of 237859 LOSS: 29.239485\n",
      "STEP 21100 of 237859 LOSS: 23.222702\n",
      "STEP 21200 of 237859 LOSS: 27.753532\n",
      "STEP 21300 of 237859 LOSS: 45.476925\n",
      "STEP 21400 of 237859 LOSS: 19.419458\n",
      "STEP 21500 of 237859 LOSS: 14.98127\n",
      "STEP 21600 of 237859 LOSS: 19.99237\n",
      "STEP 21700 of 237859 LOSS: 19.708632\n",
      "STEP 21800 of 237859 LOSS: 42.108833\n",
      "STEP 21900 of 237859 LOSS: 25.039253\n",
      "STEP 22000 of 237859 LOSS: 30.813618\n",
      "STEP 22100 of 237859 LOSS: 27.997906\n",
      "STEP 22200 of 237859 LOSS: 30.503735\n",
      "STEP 22300 of 237859 LOSS: 7.6587734\n",
      "STEP 22400 of 237859 LOSS: 42.688286\n",
      "STEP 22500 of 237859 LOSS: 33.42028\n",
      "STEP 22600 of 237859 LOSS: 25.8184\n",
      "STEP 22700 of 237859 LOSS: 28.260254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 22800 of 237859 LOSS: 40.075245\n",
      "STEP 22900 of 237859 LOSS: 34.77539\n",
      "STEP 23000 of 237859 LOSS: 27.884123\n",
      "STEP 23100 of 237859 LOSS: 17.454952\n",
      "STEP 23200 of 237859 LOSS: 29.362856\n",
      "STEP 23300 of 237859 LOSS: 44.803795\n",
      "STEP 23400 of 237859 LOSS: 9.733752\n",
      "STEP 23500 of 237859 LOSS: 38.45448\n",
      "STEP 23600 of 237859 LOSS: 28.104227\n",
      "STEP 23700 of 237859 LOSS: 26.94838\n",
      "STEP 23800 of 237859 LOSS: 21.31338\n",
      "STEP 23900 of 237859 LOSS: 26.210938\n",
      "STEP 24000 of 237859 LOSS: 17.437284\n",
      "STEP 24100 of 237859 LOSS: 36.240574\n",
      "STEP 24200 of 237859 LOSS: 26.413656\n",
      "STEP 24300 of 237859 LOSS: 18.10022\n",
      "STEP 24400 of 237859 LOSS: 26.563194\n",
      "STEP 24500 of 237859 LOSS: 21.894041\n",
      "STEP 24600 of 237859 LOSS: 8.674709\n",
      "STEP 24700 of 237859 LOSS: 18.455818\n",
      "STEP 24800 of 237859 LOSS: 36.277405\n",
      "STEP 24900 of 237859 LOSS: 30.114204\n",
      "STEP 25000 of 237859 LOSS: 24.525135\n",
      "STEP 25100 of 237859 LOSS: 23.61957\n",
      "STEP 25200 of 237859 LOSS: 19.490595\n",
      "STEP 25300 of 237859 LOSS: 34.403687\n",
      "STEP 25400 of 237859 LOSS: 20.544834\n",
      "STEP 25500 of 237859 LOSS: 17.353876\n",
      "STEP 25600 of 237859 LOSS: 36.55465\n",
      "STEP 25700 of 237859 LOSS: 7.3911\n",
      "STEP 25800 of 237859 LOSS: 27.503881\n",
      "STEP 25900 of 237859 LOSS: 52.478683\n",
      "STEP 26000 of 237859 LOSS: 22.959682\n",
      "STEP 26100 of 237859 LOSS: 19.686544\n",
      "STEP 26200 of 237859 LOSS: 23.372787\n",
      "STEP 26300 of 237859 LOSS: 12.002032\n",
      "STEP 26400 of 237859 LOSS: 23.763248\n",
      "STEP 26500 of 237859 LOSS: 26.627571\n",
      "STEP 26600 of 237859 LOSS: 17.293823\n",
      "STEP 26700 of 237859 LOSS: 22.337923\n",
      "STEP 26800 of 237859 LOSS: 17.345253\n",
      "STEP 26900 of 237859 LOSS: 31.294554\n",
      "STEP 27000 of 237859 LOSS: 20.30505\n",
      "STEP 27100 of 237859 LOSS: 35.77374\n",
      "STEP 27200 of 237859 LOSS: 19.43501\n",
      "STEP 27300 of 237859 LOSS: 20.883175\n",
      "STEP 27400 of 237859 LOSS: 18.859709\n",
      "STEP 27500 of 237859 LOSS: 14.936858\n",
      "STEP 27600 of 237859 LOSS: 20.4887\n",
      "STEP 27700 of 237859 LOSS: 10.285654\n",
      "STEP 27800 of 237859 LOSS: 24.45596\n",
      "STEP 27900 of 237859 LOSS: 53.274555\n",
      "STEP 28000 of 237859 LOSS: 20.058697\n",
      "STEP 28100 of 237859 LOSS: 21.773346\n",
      "STEP 28200 of 237859 LOSS: 20.796074\n",
      "STEP 28300 of 237859 LOSS: 29.068363\n",
      "STEP 28400 of 237859 LOSS: 19.281456\n",
      "STEP 28500 of 237859 LOSS: 11.793178\n",
      "STEP 28600 of 237859 LOSS: 20.856634\n",
      "STEP 28700 of 237859 LOSS: 18.46509\n",
      "STEP 28800 of 237859 LOSS: 11.741232\n",
      "STEP 28900 of 237859 LOSS: 23.910027\n",
      "STEP 29000 of 237859 LOSS: 18.468632\n",
      "STEP 29100 of 237859 LOSS: 12.056263\n",
      "STEP 29200 of 237859 LOSS: 17.27474\n",
      "STEP 29300 of 237859 LOSS: 25.975628\n",
      "STEP 29400 of 237859 LOSS: 8.283114\n",
      "STEP 29500 of 237859 LOSS: 12.152622\n",
      "STEP 29600 of 237859 LOSS: 10.604855\n",
      "STEP 29700 of 237859 LOSS: 22.735458\n",
      "STEP 29800 of 237859 LOSS: 10.035688\n",
      "STEP 29900 of 237859 LOSS: 15.927727\n",
      "STEP 30000 of 237859 LOSS: 16.574085\n",
      "STEP 30100 of 237859 LOSS: 31.296463\n",
      "STEP 30200 of 237859 LOSS: 20.625399\n",
      "STEP 30300 of 237859 LOSS: 14.241082\n",
      "STEP 30400 of 237859 LOSS: 16.56631\n",
      "STEP 30500 of 237859 LOSS: 26.976933\n",
      "STEP 30600 of 237859 LOSS: 36.825478\n",
      "STEP 30700 of 237859 LOSS: 14.472426\n",
      "STEP 30800 of 237859 LOSS: 22.97866\n",
      "STEP 30900 of 237859 LOSS: 10.7105665\n",
      "STEP 31000 of 237859 LOSS: 18.846062\n",
      "STEP 31100 of 237859 LOSS: 11.063332\n",
      "STEP 31200 of 237859 LOSS: 5.0275216\n",
      "STEP 31300 of 237859 LOSS: 17.152462\n",
      "STEP 31400 of 237859 LOSS: 16.456068\n",
      "STEP 31500 of 237859 LOSS: 34.931892\n",
      "STEP 31600 of 237859 LOSS: 21.974802\n",
      "STEP 31700 of 237859 LOSS: 27.98134\n",
      "STEP 31800 of 237859 LOSS: 25.581387\n",
      "STEP 31900 of 237859 LOSS: 33.15191\n",
      "STEP 32000 of 237859 LOSS: 24.140928\n",
      "STEP 32100 of 237859 LOSS: 11.527557\n",
      "STEP 32200 of 237859 LOSS: 15.336878\n",
      "STEP 32300 of 237859 LOSS: 17.866898\n",
      "STEP 32400 of 237859 LOSS: 15.060586\n",
      "STEP 32500 of 237859 LOSS: 11.191366\n",
      "STEP 32600 of 237859 LOSS: 19.189995\n",
      "STEP 32700 of 237859 LOSS: 15.228201\n",
      "STEP 32800 of 237859 LOSS: 10.752964\n",
      "STEP 32900 of 237859 LOSS: 9.066965\n",
      "STEP 33000 of 237859 LOSS: 17.883774\n",
      "STEP 33100 of 237859 LOSS: 24.369888\n",
      "STEP 33200 of 237859 LOSS: 18.420252\n",
      "STEP 33300 of 237859 LOSS: 21.714725\n",
      "STEP 33400 of 237859 LOSS: 27.174965\n",
      "STEP 33500 of 237859 LOSS: 21.284826\n",
      "STEP 33600 of 237859 LOSS: 38.67086\n",
      "STEP 33700 of 237859 LOSS: 20.03563\n",
      "STEP 33800 of 237859 LOSS: 31.447218\n",
      "STEP 33900 of 237859 LOSS: 35.4909\n",
      "STEP 34000 of 237859 LOSS: 12.834871\n",
      "STEP 34100 of 237859 LOSS: 14.130789\n",
      "STEP 34200 of 237859 LOSS: 11.802732\n",
      "STEP 34300 of 237859 LOSS: 21.156271\n",
      "STEP 34400 of 237859 LOSS: 6.7892447\n",
      "STEP 34500 of 237859 LOSS: 17.06113\n",
      "STEP 34600 of 237859 LOSS: 8.443687\n",
      "STEP 34700 of 237859 LOSS: 9.139926\n",
      "STEP 34800 of 237859 LOSS: 21.924206\n",
      "STEP 34900 of 237859 LOSS: 14.738557\n",
      "STEP 35000 of 237859 LOSS: 25.743755\n",
      "STEP 35100 of 237859 LOSS: 6.246795\n",
      "STEP 35200 of 237859 LOSS: 11.396643\n",
      "STEP 35300 of 237859 LOSS: 19.34108\n",
      "STEP 35400 of 237859 LOSS: 21.37473\n",
      "STEP 35500 of 237859 LOSS: 29.921043\n",
      "STEP 35600 of 237859 LOSS: 19.871363\n",
      "STEP 35700 of 237859 LOSS: 10.440347\n",
      "STEP 35800 of 237859 LOSS: 24.149078\n",
      "STEP 35900 of 237859 LOSS: 28.98658\n",
      "STEP 36000 of 237859 LOSS: 26.621132\n",
      "STEP 36100 of 237859 LOSS: 8.028179\n",
      "STEP 36200 of 237859 LOSS: 9.305617\n",
      "STEP 36300 of 237859 LOSS: 13.925568\n",
      "STEP 36400 of 237859 LOSS: 22.213675\n",
      "STEP 36500 of 237859 LOSS: 23.103048\n",
      "STEP 36600 of 237859 LOSS: 16.830048\n",
      "STEP 36700 of 237859 LOSS: 28.447506\n",
      "STEP 36800 of 237859 LOSS: 26.263157\n",
      "STEP 36900 of 237859 LOSS: 20.374407\n",
      "STEP 37000 of 237859 LOSS: 26.498156\n",
      "STEP 37100 of 237859 LOSS: 11.8966055\n",
      "STEP 37200 of 237859 LOSS: 17.073229\n",
      "STEP 37300 of 237859 LOSS: 10.828741\n",
      "STEP 37400 of 237859 LOSS: 16.372543\n",
      "STEP 37500 of 237859 LOSS: 34.386505\n",
      "STEP 37600 of 237859 LOSS: 28.2675\n",
      "STEP 37700 of 237859 LOSS: 8.375263\n",
      "STEP 37800 of 237859 LOSS: 28.830193\n",
      "STEP 37900 of 237859 LOSS: 14.835228\n",
      "STEP 38000 of 237859 LOSS: 9.533809\n",
      "STEP 38100 of 237859 LOSS: 11.974421\n",
      "STEP 38200 of 237859 LOSS: 28.033783\n",
      "STEP 38300 of 237859 LOSS: 13.290512\n",
      "STEP 38400 of 237859 LOSS: 29.892567\n",
      "STEP 38500 of 237859 LOSS: 7.9782906\n",
      "STEP 38600 of 237859 LOSS: 25.297438\n",
      "STEP 38700 of 237859 LOSS: 14.972946\n",
      "STEP 38800 of 237859 LOSS: 19.679474\n",
      "STEP 38900 of 237859 LOSS: 18.86478\n",
      "STEP 39000 of 237859 LOSS: 16.777699\n",
      "STEP 39100 of 237859 LOSS: 12.976201\n",
      "STEP 39200 of 237859 LOSS: 20.452507\n",
      "STEP 39300 of 237859 LOSS: 5.9403734\n",
      "STEP 39400 of 237859 LOSS: 10.499523\n",
      "STEP 39500 of 237859 LOSS: 15.100745\n",
      "STEP 39600 of 237859 LOSS: 6.31011\n",
      "STEP 39700 of 237859 LOSS: 7.588093\n",
      "STEP 39800 of 237859 LOSS: 22.37675\n",
      "STEP 39900 of 237859 LOSS: 9.977846\n",
      "STEP 40000 of 237859 LOSS: 18.686918\n",
      "STEP 40100 of 237859 LOSS: 13.403008\n",
      "STEP 40200 of 237859 LOSS: 8.954664\n",
      "STEP 40300 of 237859 LOSS: 13.294447\n",
      "STEP 40400 of 237859 LOSS: 10.671949\n",
      "STEP 40500 of 237859 LOSS: 11.547703\n",
      "STEP 40600 of 237859 LOSS: 18.801931\n",
      "STEP 40700 of 237859 LOSS: 22.390324\n",
      "STEP 40800 of 237859 LOSS: 9.028346\n",
      "STEP 40900 of 237859 LOSS: 16.843285\n",
      "STEP 41000 of 237859 LOSS: 19.676506\n",
      "STEP 41100 of 237859 LOSS: 10.6038265\n",
      "STEP 41200 of 237859 LOSS: 10.383743\n",
      "STEP 41300 of 237859 LOSS: 27.211193\n",
      "STEP 41400 of 237859 LOSS: 7.76527\n",
      "STEP 41500 of 237859 LOSS: 6.835576\n",
      "STEP 41600 of 237859 LOSS: 6.947554\n",
      "STEP 41700 of 237859 LOSS: 16.60196\n",
      "STEP 41800 of 237859 LOSS: 24.522373\n",
      "STEP 41900 of 237859 LOSS: 22.744602\n",
      "STEP 42000 of 237859 LOSS: 24.969898\n",
      "STEP 42100 of 237859 LOSS: 16.197409\n",
      "STEP 42200 of 237859 LOSS: 14.340888\n",
      "STEP 42300 of 237859 LOSS: 6.3873215\n",
      "STEP 42400 of 237859 LOSS: 25.545498\n",
      "STEP 42500 of 237859 LOSS: 7.396175\n",
      "STEP 42600 of 237859 LOSS: 8.818368\n",
      "STEP 42700 of 237859 LOSS: 5.3576503\n",
      "STEP 42800 of 237859 LOSS: 26.970932\n",
      "STEP 42900 of 237859 LOSS: 11.215088\n",
      "STEP 43000 of 237859 LOSS: 6.9459934\n",
      "STEP 43100 of 237859 LOSS: 22.184383\n",
      "STEP 43200 of 237859 LOSS: 14.728816\n",
      "STEP 43300 of 237859 LOSS: 22.709927\n",
      "STEP 43400 of 237859 LOSS: 8.417681\n",
      "STEP 43500 of 237859 LOSS: 3.9735005\n",
      "STEP 43600 of 237859 LOSS: 23.664165\n",
      "STEP 43700 of 237859 LOSS: 11.296851\n",
      "STEP 43800 of 237859 LOSS: 14.290745\n",
      "STEP 43900 of 237859 LOSS: 17.277803\n",
      "STEP 44000 of 237859 LOSS: 17.83207\n",
      "STEP 44100 of 237859 LOSS: 17.069464\n",
      "STEP 44200 of 237859 LOSS: 23.967096\n",
      "STEP 44300 of 237859 LOSS: 8.672503\n",
      "STEP 44400 of 237859 LOSS: 10.977795\n",
      "STEP 44500 of 237859 LOSS: 5.3277235\n",
      "STEP 44600 of 237859 LOSS: 16.51079\n",
      "STEP 44700 of 237859 LOSS: 18.696943\n",
      "STEP 44800 of 237859 LOSS: 8.024375\n",
      "STEP 44900 of 237859 LOSS: 29.502577\n",
      "STEP 45000 of 237859 LOSS: 20.562098\n",
      "STEP 45100 of 237859 LOSS: 16.549019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 45200 of 237859 LOSS: 6.328998\n",
      "STEP 45300 of 237859 LOSS: 14.049558\n",
      "STEP 45400 of 237859 LOSS: 8.131068\n",
      "STEP 45500 of 237859 LOSS: 12.916044\n",
      "STEP 45600 of 237859 LOSS: 10.201239\n",
      "STEP 45700 of 237859 LOSS: 19.094028\n",
      "STEP 45800 of 237859 LOSS: 10.534662\n",
      "STEP 45900 of 237859 LOSS: 7.0801635\n",
      "STEP 46000 of 237859 LOSS: 8.7791605\n",
      "STEP 46100 of 237859 LOSS: 12.145319\n",
      "STEP 46200 of 237859 LOSS: 19.686707\n",
      "STEP 46300 of 237859 LOSS: 5.8130836\n",
      "STEP 46400 of 237859 LOSS: 10.713291\n",
      "STEP 46500 of 237859 LOSS: 18.538212\n",
      "STEP 46600 of 237859 LOSS: 9.202403\n",
      "STEP 46700 of 237859 LOSS: 14.320734\n",
      "STEP 46800 of 237859 LOSS: 9.684617\n",
      "STEP 46900 of 237859 LOSS: 15.417639\n",
      "STEP 47000 of 237859 LOSS: 5.9926124\n",
      "STEP 47100 of 237859 LOSS: 21.151455\n",
      "STEP 47200 of 237859 LOSS: 5.41206\n",
      "STEP 47300 of 237859 LOSS: 13.184311\n",
      "STEP 47400 of 237859 LOSS: 8.946079\n",
      "STEP 47500 of 237859 LOSS: 15.963872\n",
      "STEP 47600 of 237859 LOSS: 8.481514\n",
      "STEP 47700 of 237859 LOSS: 12.8539095\n",
      "STEP 47800 of 237859 LOSS: 10.984953\n",
      "STEP 47900 of 237859 LOSS: 26.006207\n",
      "STEP 48000 of 237859 LOSS: 11.736288\n",
      "STEP 48100 of 237859 LOSS: 22.503605\n",
      "STEP 48200 of 237859 LOSS: 19.974527\n",
      "STEP 48300 of 237859 LOSS: 11.57111\n",
      "STEP 48400 of 237859 LOSS: 17.37266\n",
      "STEP 48500 of 237859 LOSS: 6.609749\n",
      "STEP 48600 of 237859 LOSS: 12.830607\n",
      "STEP 48700 of 237859 LOSS: 25.568382\n",
      "STEP 48800 of 237859 LOSS: 7.755174\n",
      "STEP 48900 of 237859 LOSS: 12.385217\n",
      "STEP 49000 of 237859 LOSS: 4.6577935\n",
      "STEP 49100 of 237859 LOSS: 19.39307\n",
      "STEP 49200 of 237859 LOSS: 10.272325\n",
      "STEP 49300 of 237859 LOSS: 9.607964\n",
      "STEP 49400 of 237859 LOSS: 9.320698\n",
      "STEP 49500 of 237859 LOSS: 5.4825583\n",
      "STEP 49600 of 237859 LOSS: 6.4165335\n",
      "STEP 49700 of 237859 LOSS: 5.1664176\n",
      "STEP 49800 of 237859 LOSS: 9.231759\n",
      "STEP 49900 of 237859 LOSS: 14.779516\n",
      "STEP 50000 of 237859 LOSS: 7.5820518\n",
      "STEP 50100 of 237859 LOSS: 7.8243384\n",
      "STEP 50200 of 237859 LOSS: 10.626316\n",
      "STEP 50300 of 237859 LOSS: 12.50022\n",
      "STEP 50400 of 237859 LOSS: 11.15234\n",
      "STEP 50500 of 237859 LOSS: 22.517754\n",
      "STEP 50600 of 237859 LOSS: 10.775315\n",
      "STEP 50700 of 237859 LOSS: 9.271883\n",
      "STEP 50800 of 237859 LOSS: 23.872595\n",
      "STEP 50900 of 237859 LOSS: 8.640764\n",
      "STEP 51000 of 237859 LOSS: 6.117016\n",
      "STEP 51100 of 237859 LOSS: 26.68198\n",
      "STEP 51200 of 237859 LOSS: 14.433208\n",
      "STEP 51300 of 237859 LOSS: 15.181704\n",
      "STEP 51400 of 237859 LOSS: 6.174143\n",
      "STEP 51500 of 237859 LOSS: 10.164354\n",
      "STEP 51600 of 237859 LOSS: 12.433279\n",
      "STEP 51700 of 237859 LOSS: 7.160827\n",
      "STEP 51800 of 237859 LOSS: 8.188206\n",
      "STEP 51900 of 237859 LOSS: 20.468641\n",
      "STEP 52000 of 237859 LOSS: 8.223441\n",
      "STEP 52100 of 237859 LOSS: 10.623767\n",
      "STEP 52200 of 237859 LOSS: 8.625875\n",
      "STEP 52300 of 237859 LOSS: 14.3212185\n",
      "STEP 52400 of 237859 LOSS: 22.918423\n",
      "STEP 52500 of 237859 LOSS: 6.955085\n",
      "STEP 52600 of 237859 LOSS: 11.203274\n",
      "STEP 52700 of 237859 LOSS: 14.33744\n",
      "STEP 52800 of 237859 LOSS: 12.974393\n",
      "STEP 52900 of 237859 LOSS: 4.571391\n",
      "STEP 53000 of 237859 LOSS: 4.8189306\n",
      "STEP 53100 of 237859 LOSS: 10.480061\n",
      "STEP 53200 of 237859 LOSS: 20.55658\n",
      "STEP 53300 of 237859 LOSS: 7.632182\n",
      "STEP 53400 of 237859 LOSS: 8.598005\n",
      "STEP 53500 of 237859 LOSS: 11.437931\n",
      "STEP 53600 of 237859 LOSS: 14.217714\n",
      "STEP 53700 of 237859 LOSS: 24.160315\n",
      "STEP 53800 of 237859 LOSS: 7.8254056\n",
      "STEP 53900 of 237859 LOSS: 17.02012\n",
      "STEP 54000 of 237859 LOSS: 22.030973\n",
      "STEP 54100 of 237859 LOSS: 18.358139\n",
      "STEP 54200 of 237859 LOSS: 14.932926\n",
      "STEP 54300 of 237859 LOSS: 9.077268\n",
      "STEP 54400 of 237859 LOSS: 15.463599\n",
      "STEP 54500 of 237859 LOSS: 12.657638\n",
      "STEP 54600 of 237859 LOSS: 12.390863\n",
      "STEP 54700 of 237859 LOSS: 16.226423\n",
      "STEP 54800 of 237859 LOSS: 7.6527104\n",
      "STEP 54900 of 237859 LOSS: 12.862606\n",
      "STEP 55000 of 237859 LOSS: 11.764119\n",
      "STEP 55100 of 237859 LOSS: 10.895694\n",
      "STEP 55200 of 237859 LOSS: 7.9834046\n",
      "STEP 55300 of 237859 LOSS: 28.001528\n",
      "STEP 55400 of 237859 LOSS: 8.919258\n",
      "STEP 55500 of 237859 LOSS: 9.133772\n",
      "STEP 55600 of 237859 LOSS: 7.872757\n",
      "STEP 55700 of 237859 LOSS: 10.909187\n",
      "STEP 55800 of 237859 LOSS: 5.507289\n",
      "STEP 55900 of 237859 LOSS: 14.512968\n",
      "STEP 56000 of 237859 LOSS: 16.15379\n",
      "STEP 56100 of 237859 LOSS: 29.493092\n",
      "STEP 56200 of 237859 LOSS: 9.511194\n",
      "STEP 56300 of 237859 LOSS: 19.49936\n",
      "STEP 56400 of 237859 LOSS: 3.8384461\n",
      "STEP 56500 of 237859 LOSS: 6.904028\n",
      "STEP 56600 of 237859 LOSS: 14.9591675\n",
      "STEP 56700 of 237859 LOSS: 4.68621\n",
      "STEP 56800 of 237859 LOSS: 18.235664\n",
      "STEP 56900 of 237859 LOSS: 5.975218\n",
      "STEP 57000 of 237859 LOSS: 7.7255945\n",
      "STEP 57100 of 237859 LOSS: 11.055949\n",
      "STEP 57200 of 237859 LOSS: 7.434408\n",
      "STEP 57300 of 237859 LOSS: 10.507824\n",
      "STEP 57400 of 237859 LOSS: 11.742735\n",
      "STEP 57500 of 237859 LOSS: 11.435395\n",
      "STEP 57600 of 237859 LOSS: 9.732452\n",
      "STEP 57700 of 237859 LOSS: 6.557581\n",
      "STEP 57800 of 237859 LOSS: 14.649435\n",
      "STEP 57900 of 237859 LOSS: 13.130096\n",
      "STEP 58000 of 237859 LOSS: 4.6475163\n",
      "STEP 58100 of 237859 LOSS: 12.119636\n",
      "STEP 58200 of 237859 LOSS: 19.706724\n",
      "STEP 58300 of 237859 LOSS: 6.308983\n",
      "STEP 58400 of 237859 LOSS: 8.887885\n",
      "STEP 58500 of 237859 LOSS: 18.921803\n",
      "STEP 58600 of 237859 LOSS: 13.260709\n",
      "STEP 58700 of 237859 LOSS: 10.543974\n",
      "STEP 58800 of 237859 LOSS: 16.694508\n",
      "STEP 58900 of 237859 LOSS: 11.894091\n",
      "STEP 59000 of 237859 LOSS: 12.249965\n",
      "STEP 59100 of 237859 LOSS: 4.4187717\n",
      "STEP 59200 of 237859 LOSS: 6.1054935\n",
      "STEP 59300 of 237859 LOSS: 16.639935\n",
      "STEP 59400 of 237859 LOSS: 8.120135\n",
      "STEP 59500 of 237859 LOSS: 6.1574287\n",
      "STEP 59600 of 237859 LOSS: 5.320908\n",
      "STEP 59700 of 237859 LOSS: 8.580998\n",
      "STEP 59800 of 237859 LOSS: 6.113452\n",
      "STEP 59900 of 237859 LOSS: 8.3221035\n",
      "STEP 60000 of 237859 LOSS: 6.205676\n",
      "STEP 60100 of 237859 LOSS: 8.559038\n",
      "STEP 60200 of 237859 LOSS: 11.431903\n",
      "STEP 60300 of 237859 LOSS: 11.249497\n",
      "STEP 60400 of 237859 LOSS: 6.1096883\n",
      "STEP 60500 of 237859 LOSS: 7.4357243\n",
      "STEP 60600 of 237859 LOSS: 5.6562767\n",
      "STEP 60700 of 237859 LOSS: 8.95471\n",
      "STEP 60800 of 237859 LOSS: 7.24834\n",
      "STEP 60900 of 237859 LOSS: 14.270879\n",
      "STEP 61000 of 237859 LOSS: 6.71254\n",
      "STEP 61100 of 237859 LOSS: 12.223029\n",
      "STEP 61200 of 237859 LOSS: 20.11814\n",
      "STEP 61300 of 237859 LOSS: 8.898636\n",
      "STEP 61400 of 237859 LOSS: 5.1299224\n",
      "STEP 61500 of 237859 LOSS: 5.691452\n",
      "STEP 61600 of 237859 LOSS: 20.46349\n",
      "STEP 61700 of 237859 LOSS: 8.330603\n",
      "STEP 61800 of 237859 LOSS: 13.080033\n",
      "STEP 61900 of 237859 LOSS: 9.969383\n",
      "STEP 62000 of 237859 LOSS: 9.528105\n",
      "STEP 62100 of 237859 LOSS: 12.962775\n",
      "STEP 62200 of 237859 LOSS: 10.978285\n",
      "STEP 62300 of 237859 LOSS: 15.896996\n",
      "STEP 62400 of 237859 LOSS: 14.364212\n",
      "STEP 62500 of 237859 LOSS: 19.095646\n",
      "STEP 62600 of 237859 LOSS: 10.230087\n",
      "STEP 62700 of 237859 LOSS: 5.330015\n",
      "STEP 62800 of 237859 LOSS: 7.771981\n",
      "STEP 62900 of 237859 LOSS: 6.062537\n",
      "STEP 63000 of 237859 LOSS: 5.2865925\n",
      "STEP 63100 of 237859 LOSS: 12.403803\n",
      "STEP 63200 of 237859 LOSS: 16.20632\n",
      "STEP 63300 of 237859 LOSS: 10.012087\n",
      "STEP 63400 of 237859 LOSS: 13.315943\n",
      "STEP 63500 of 237859 LOSS: 4.0909033\n",
      "STEP 63600 of 237859 LOSS: 4.8608427\n",
      "STEP 63700 of 237859 LOSS: 18.894445\n",
      "STEP 63800 of 237859 LOSS: 16.039806\n",
      "STEP 63900 of 237859 LOSS: 24.541546\n",
      "STEP 64000 of 237859 LOSS: 8.180281\n",
      "STEP 64100 of 237859 LOSS: 8.932051\n",
      "STEP 64200 of 237859 LOSS: 10.641886\n",
      "STEP 64300 of 237859 LOSS: 11.2659\n",
      "STEP 64400 of 237859 LOSS: 6.6773167\n",
      "STEP 64500 of 237859 LOSS: 7.6164904\n",
      "STEP 64600 of 237859 LOSS: 8.516142\n",
      "STEP 64700 of 237859 LOSS: 18.392025\n",
      "STEP 64800 of 237859 LOSS: 12.24353\n",
      "STEP 64900 of 237859 LOSS: 6.9041505\n",
      "STEP 65000 of 237859 LOSS: 4.8193593\n",
      "STEP 65100 of 237859 LOSS: 5.4108667\n",
      "STEP 65200 of 237859 LOSS: 15.602215\n",
      "STEP 65300 of 237859 LOSS: 12.011535\n",
      "STEP 65400 of 237859 LOSS: 11.62178\n",
      "STEP 65500 of 237859 LOSS: 16.275936\n",
      "STEP 65600 of 237859 LOSS: 12.626772\n",
      "STEP 65700 of 237859 LOSS: 8.193257\n",
      "STEP 65800 of 237859 LOSS: 5.2536864\n",
      "STEP 65900 of 237859 LOSS: 8.257353\n",
      "STEP 66000 of 237859 LOSS: 6.4169083\n",
      "STEP 66100 of 237859 LOSS: 10.911062\n",
      "STEP 66200 of 237859 LOSS: 14.579092\n",
      "STEP 66300 of 237859 LOSS: 5.6238194\n",
      "STEP 66400 of 237859 LOSS: 5.558324\n",
      "STEP 66500 of 237859 LOSS: 7.9044743\n",
      "STEP 66600 of 237859 LOSS: 19.590403\n",
      "STEP 66700 of 237859 LOSS: 12.606101\n",
      "STEP 66800 of 237859 LOSS: 5.219903\n",
      "STEP 66900 of 237859 LOSS: 6.002305\n",
      "STEP 67000 of 237859 LOSS: 8.895829\n",
      "STEP 67100 of 237859 LOSS: 18.383654\n",
      "STEP 67200 of 237859 LOSS: 11.834877\n",
      "STEP 67300 of 237859 LOSS: 20.406483\n",
      "STEP 67400 of 237859 LOSS: 4.797647\n",
      "STEP 67500 of 237859 LOSS: 14.375097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 67600 of 237859 LOSS: 12.250902\n",
      "STEP 67700 of 237859 LOSS: 6.624948\n",
      "STEP 67800 of 237859 LOSS: 10.603014\n",
      "STEP 67900 of 237859 LOSS: 7.634921\n",
      "STEP 68000 of 237859 LOSS: 23.47526\n",
      "STEP 68100 of 237859 LOSS: 4.048181\n",
      "STEP 68200 of 237859 LOSS: 8.731743\n",
      "STEP 68300 of 237859 LOSS: 5.2625036\n",
      "STEP 68400 of 237859 LOSS: 7.5406055\n",
      "STEP 68500 of 237859 LOSS: 12.982437\n",
      "STEP 68600 of 237859 LOSS: 13.163601\n",
      "STEP 68700 of 237859 LOSS: 3.9313183\n",
      "STEP 68800 of 237859 LOSS: 11.662472\n",
      "STEP 68900 of 237859 LOSS: 10.884954\n",
      "STEP 69000 of 237859 LOSS: 14.814308\n",
      "STEP 69100 of 237859 LOSS: 6.8969946\n",
      "STEP 69200 of 237859 LOSS: 18.351187\n",
      "STEP 69300 of 237859 LOSS: 10.136721\n",
      "STEP 69400 of 237859 LOSS: 6.0792894\n",
      "STEP 69500 of 237859 LOSS: 18.797161\n",
      "STEP 69600 of 237859 LOSS: 8.628956\n",
      "STEP 69700 of 237859 LOSS: 6.4680643\n",
      "STEP 69800 of 237859 LOSS: 22.404127\n",
      "STEP 69900 of 237859 LOSS: 9.078835\n",
      "STEP 70000 of 237859 LOSS: 9.995186\n",
      "STEP 70100 of 237859 LOSS: 7.1165094\n",
      "STEP 70200 of 237859 LOSS: 5.609754\n",
      "STEP 70300 of 237859 LOSS: 10.319352\n",
      "STEP 70400 of 237859 LOSS: 4.96811\n",
      "STEP 70500 of 237859 LOSS: 14.761881\n",
      "STEP 70600 of 237859 LOSS: 10.092409\n",
      "STEP 70700 of 237859 LOSS: 11.063707\n",
      "STEP 70800 of 237859 LOSS: 13.03723\n",
      "STEP 70900 of 237859 LOSS: 7.466916\n",
      "STEP 71000 of 237859 LOSS: 8.528413\n",
      "STEP 71100 of 237859 LOSS: 4.4250484\n",
      "STEP 71200 of 237859 LOSS: 8.564335\n",
      "STEP 71300 of 237859 LOSS: 5.323002\n",
      "STEP 71400 of 237859 LOSS: 11.289309\n",
      "STEP 71500 of 237859 LOSS: 6.4140463\n",
      "STEP 71600 of 237859 LOSS: 8.738407\n",
      "STEP 71700 of 237859 LOSS: 9.288254\n",
      "STEP 71800 of 237859 LOSS: 5.2900534\n",
      "STEP 71900 of 237859 LOSS: 13.752901\n",
      "STEP 72000 of 237859 LOSS: 25.42485\n",
      "STEP 72100 of 237859 LOSS: 10.724035\n",
      "STEP 72200 of 237859 LOSS: 8.774131\n",
      "STEP 72300 of 237859 LOSS: 6.355795\n",
      "STEP 72400 of 237859 LOSS: 7.5707192\n",
      "STEP 72500 of 237859 LOSS: 6.2479854\n",
      "STEP 72600 of 237859 LOSS: 8.834535\n",
      "STEP 72700 of 237859 LOSS: 4.663048\n",
      "STEP 72800 of 237859 LOSS: 8.389925\n",
      "STEP 72900 of 237859 LOSS: 15.310771\n",
      "STEP 73000 of 237859 LOSS: 18.396694\n",
      "STEP 73100 of 237859 LOSS: 4.4675636\n",
      "STEP 73200 of 237859 LOSS: 24.095867\n",
      "STEP 73300 of 237859 LOSS: 15.151962\n",
      "STEP 73400 of 237859 LOSS: 19.14997\n",
      "STEP 73500 of 237859 LOSS: 4.1623945\n",
      "STEP 73600 of 237859 LOSS: 14.273981\n",
      "STEP 73700 of 237859 LOSS: 15.120308\n",
      "STEP 73800 of 237859 LOSS: 4.0965314\n",
      "STEP 73900 of 237859 LOSS: 13.441402\n",
      "STEP 74000 of 237859 LOSS: 9.317026\n",
      "STEP 74100 of 237859 LOSS: 8.8816805\n",
      "STEP 74200 of 237859 LOSS: 9.32716\n",
      "STEP 74300 of 237859 LOSS: 4.866246\n",
      "STEP 74400 of 237859 LOSS: 7.5814247\n",
      "STEP 74500 of 237859 LOSS: 9.777725\n",
      "STEP 74600 of 237859 LOSS: 7.7845483\n",
      "STEP 74700 of 237859 LOSS: 16.077662\n",
      "STEP 74800 of 237859 LOSS: 9.723066\n",
      "STEP 74900 of 237859 LOSS: 5.466769\n",
      "STEP 75000 of 237859 LOSS: 6.8324976\n",
      "STEP 75100 of 237859 LOSS: 12.951582\n",
      "STEP 75200 of 237859 LOSS: 11.4307785\n",
      "STEP 75300 of 237859 LOSS: 8.782796\n",
      "STEP 75400 of 237859 LOSS: 6.1416283\n",
      "STEP 75500 of 237859 LOSS: 9.067525\n",
      "STEP 75600 of 237859 LOSS: 12.967508\n",
      "STEP 75700 of 237859 LOSS: 6.946563\n",
      "STEP 75800 of 237859 LOSS: 12.905981\n",
      "STEP 75900 of 237859 LOSS: 20.155306\n",
      "STEP 76000 of 237859 LOSS: 7.935657\n",
      "STEP 76100 of 237859 LOSS: 5.670145\n",
      "STEP 76200 of 237859 LOSS: 8.430454\n",
      "STEP 76300 of 237859 LOSS: 20.546814\n",
      "STEP 76400 of 237859 LOSS: 23.397892\n",
      "STEP 76500 of 237859 LOSS: 10.683439\n",
      "STEP 76600 of 237859 LOSS: 7.5153155\n",
      "STEP 76700 of 237859 LOSS: 5.0613637\n",
      "STEP 76800 of 237859 LOSS: 8.633799\n",
      "STEP 76900 of 237859 LOSS: 7.7766666\n",
      "STEP 77000 of 237859 LOSS: 4.55119\n",
      "STEP 77100 of 237859 LOSS: 12.956427\n",
      "STEP 77200 of 237859 LOSS: 8.571917\n",
      "STEP 77300 of 237859 LOSS: 8.915839\n",
      "STEP 77400 of 237859 LOSS: 5.951792\n",
      "STEP 77500 of 237859 LOSS: 4.2080274\n",
      "STEP 77600 of 237859 LOSS: 3.9806871\n",
      "STEP 77700 of 237859 LOSS: 11.224739\n",
      "STEP 77800 of 237859 LOSS: 4.2927566\n",
      "STEP 77900 of 237859 LOSS: 5.0100803\n",
      "STEP 78000 of 237859 LOSS: 9.321794\n",
      "STEP 78100 of 237859 LOSS: 7.1568604\n",
      "STEP 78200 of 237859 LOSS: 11.044062\n",
      "STEP 78300 of 237859 LOSS: 16.819473\n",
      "STEP 78400 of 237859 LOSS: 6.5303845\n",
      "STEP 78500 of 237859 LOSS: 12.126782\n",
      "STEP 78600 of 237859 LOSS: 4.892868\n",
      "STEP 78700 of 237859 LOSS: 9.312387\n",
      "STEP 78800 of 237859 LOSS: 6.330562\n",
      "STEP 78900 of 237859 LOSS: 13.896053\n",
      "STEP 79000 of 237859 LOSS: 9.269018\n",
      "STEP 79100 of 237859 LOSS: 5.5771394\n",
      "STEP 79200 of 237859 LOSS: 17.904263\n",
      "STEP 79300 of 237859 LOSS: 4.591371\n",
      "STEP 79400 of 237859 LOSS: 19.554598\n",
      "STEP 79500 of 237859 LOSS: 13.038561\n",
      "STEP 79600 of 237859 LOSS: 5.2891483\n",
      "STEP 79700 of 237859 LOSS: 8.046396\n",
      "STEP 79800 of 237859 LOSS: 7.7734723\n",
      "STEP 79900 of 237859 LOSS: 4.268089\n",
      "STEP 80000 of 237859 LOSS: 30.555027\n",
      "STEP 80100 of 237859 LOSS: 5.9782968\n",
      "STEP 80200 of 237859 LOSS: 13.034168\n",
      "STEP 80300 of 237859 LOSS: 6.6009264\n",
      "STEP 80400 of 237859 LOSS: 6.849358\n",
      "STEP 80500 of 237859 LOSS: 9.640786\n",
      "STEP 80600 of 237859 LOSS: 8.045662\n",
      "STEP 80700 of 237859 LOSS: 6.4159603\n",
      "STEP 80800 of 237859 LOSS: 7.926972\n",
      "STEP 80900 of 237859 LOSS: 5.1634793\n",
      "STEP 81000 of 237859 LOSS: 4.8762236\n",
      "STEP 81100 of 237859 LOSS: 4.4648476\n",
      "STEP 81200 of 237859 LOSS: 4.0453644\n",
      "STEP 81300 of 237859 LOSS: 5.339074\n",
      "STEP 81400 of 237859 LOSS: 8.791436\n",
      "STEP 81500 of 237859 LOSS: 6.0253205\n",
      "STEP 81600 of 237859 LOSS: 8.23476\n",
      "STEP 81700 of 237859 LOSS: 8.956397\n",
      "STEP 81800 of 237859 LOSS: 4.6830225\n",
      "STEP 81900 of 237859 LOSS: 4.4723644\n",
      "STEP 82000 of 237859 LOSS: 17.216267\n",
      "STEP 82100 of 237859 LOSS: 16.268818\n",
      "STEP 82200 of 237859 LOSS: 7.175307\n",
      "STEP 82300 of 237859 LOSS: 4.8914156\n",
      "STEP 82400 of 237859 LOSS: 8.178633\n",
      "STEP 82500 of 237859 LOSS: 8.905075\n",
      "STEP 82600 of 237859 LOSS: 8.170702\n",
      "STEP 82700 of 237859 LOSS: 9.020235\n",
      "STEP 82800 of 237859 LOSS: 7.0309505\n",
      "STEP 82900 of 237859 LOSS: 10.417371\n",
      "STEP 83000 of 237859 LOSS: 6.5583653\n",
      "STEP 83100 of 237859 LOSS: 4.9519863\n",
      "STEP 83200 of 237859 LOSS: 8.755935\n",
      "STEP 83300 of 237859 LOSS: 12.612064\n",
      "STEP 83400 of 237859 LOSS: 4.996106\n",
      "STEP 83500 of 237859 LOSS: 5.6573277\n",
      "STEP 83600 of 237859 LOSS: 13.664824\n",
      "STEP 83700 of 237859 LOSS: 4.1897717\n",
      "STEP 83800 of 237859 LOSS: 3.307652\n",
      "STEP 83900 of 237859 LOSS: 8.86359\n",
      "STEP 84000 of 237859 LOSS: 5.0447025\n",
      "STEP 84100 of 237859 LOSS: 5.1459394\n",
      "STEP 84200 of 237859 LOSS: 7.5604424\n",
      "STEP 84300 of 237859 LOSS: 11.617985\n",
      "STEP 84400 of 237859 LOSS: 9.1462555\n",
      "STEP 84500 of 237859 LOSS: 14.276045\n",
      "STEP 84600 of 237859 LOSS: 6.779272\n",
      "STEP 84700 of 237859 LOSS: 6.687251\n",
      "STEP 84800 of 237859 LOSS: 6.0533476\n",
      "STEP 84900 of 237859 LOSS: 7.150126\n",
      "STEP 85000 of 237859 LOSS: 11.878755\n",
      "STEP 85100 of 237859 LOSS: 16.810818\n",
      "STEP 85200 of 237859 LOSS: 10.2359085\n",
      "STEP 85300 of 237859 LOSS: 8.673084\n",
      "STEP 85400 of 237859 LOSS: 9.791315\n",
      "STEP 85500 of 237859 LOSS: 4.313388\n",
      "STEP 85600 of 237859 LOSS: 4.993741\n",
      "STEP 85700 of 237859 LOSS: 7.320526\n",
      "STEP 85800 of 237859 LOSS: 4.742819\n",
      "STEP 85900 of 237859 LOSS: 5.1190443\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2a2439fd8286>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer, loss, x, y, sess = word_to_vec()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(pivot_words, target_words)\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "\n",
    "#Num batches in training set\n",
    "num_batches = len(X_train) // batch_size\n",
    "\n",
    "#create saver to save our weights\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    for i in range(num_batches):\n",
    "        if i != range(num_batches-1):\n",
    "            x_batch = X_train[i*batch_size:i * batch_size + batch_size]\n",
    "            y_batch = y_train[i*batch_size:i * batch_size + batch_size]\n",
    "        else:\n",
    "            x_batch = X_train[i*batch_size:]\n",
    "            y_batch = y_train[i*batch_size:]\n",
    "\n",
    "        _, l = sess.run([optimizer, loss], feed_dict = {x: x_batch, y: y_batch})\n",
    "\n",
    "        if i>0 and i %100 == 0:\n",
    "            print(\"STEP\", i, \"of\", num_batches, \"LOSS:\", l)\n",
    "    save_path = saver.save(sess, \"./word2vec_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
